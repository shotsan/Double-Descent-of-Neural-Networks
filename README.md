# Universal Function Approximation

We know neural networks have this amazing characteristic: Function approximation.
However, it is hard to find the optimal neurons in the architecture. 


## Deep double descent

First observed, by [Preetham et. al](https://arxiv.org/abs/1912.02292), also reported at [Open AI](https://openai.com/research/deep-double-descent)
Over-parameterization or over-fitting is expected to increase the test error, however in case of neural networks, as we increase the number of parameters
test error decreases, which is great as we need not worry much about optimizing the number of neurons in the architecture.


# Notebook
This [Notebook](neural_networks.ipynb) has code to define any function, and visualize the approximation of Neural Networks and
also observe descent phenomenon of neural networks.


#### Disclaimer: Please feel free to create an issue if find any bugs
